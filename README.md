# GENERATIVE-AI_PRACTICALS

üåü Generative AI Assignment Repository This repository includes a collection of hands-on assignments focused on Generative AI, Prompt Engineering, Multimodal Systems, and Creative AI Applications.

üìå List of Assignments 1Ô∏è‚É£ Probabilistic Text Generator using Markov Chains Goal: Implement a simple text generation model using Markov Chains to simulate language modeling behavior. Key Concepts:

Probability transitions

State-space modeling

Random sampling of next words

2Ô∏è‚É£ Prompt Engineering Approaches Goal: Implement and compare the following prompt engineering strategies:

Interview-style prompting

Chain of Thought (CoT)

Tree of Thought (ToT)

Zero-shot vs Few-shot prompting

Deliverables:

Comparison metrics (accuracy, coherence, reasoning)

Use cases and application analysis

3Ô∏è‚É£ Fine-tune GPT/GPT-2 for Story Generation Goal: Fine-tune a GPT/GPT-2 model on a dataset of fictional or fairy tale stories to generate creative outputs. Tools:

Hugging Face Transformers

Custom dataset (JSON or TXT)

Trainer API or manual training loop

4Ô∏è‚É£ QA Chatbot using Pre-trained Language Model Goal: Build a simple Question-Answering Chatbot using models like BERT, T5, or GPT. Capabilities:

Answering factual questions

Using pre-trained knowledge

Text input/output interface

5Ô∏è‚É£ Creative Artwork using Stable Diffusion Goal: Use Stable Diffusion to generate high-quality art from text prompts.

Load pre-trained model

Generate images using custom prompts

Experiment with prompt engineering for artistic variation

6Ô∏è‚É£ Text-to-Image Pipeline using DALL¬∑E Goal: Create a text-to-image generation pipeline using OpenAI's DALL¬∑E or similar pre-trained models. Pipeline Features:

Prompt input interface

Image rendering

Image download or preview

7Ô∏è‚É£ Multimodal Image Captioning using CLIP Goal: Build a simple image captioning system using CLIP (Contrastive Language‚ÄìImage Pretraining). Steps:

Input image ‚Üí extract features

Generate caption using text encoder

Optional: Integrate with GPT for enhanced captioning

8Ô∏è‚É£ Text-to-Speech Generation using Tacotron or WaveNet Goal: Convert written text into natural-sounding speech using models like Tacotron 2 or WaveNet. Implementation:

Preprocessing (phonemes or characters)

Spectrogram generation

Vocoding with WaveNet or similar
